{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo for visualisation of crop type and yield data\n",
    "import warnings\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "import rasterio as rio\n",
    "import os\n",
    "\n",
    "# 3D stuff\n",
    "from IPython.core.display import display\n",
    "import json\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "from shapely.geometry import Point  # Point class\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    'Baumacker', 'D8', 'Dichtlacker', 'Heindlacker', 'Heng', 'Holzacker',\n",
    "    'Neulandsiedlung', 'Itzling2', 'Itzling5', 'Itzling6', 'Schluetterfabrik',\n",
    "    'Thalhausen138', 'Thalhausen141', 'Voettingerfeld'\n",
    "]\n",
    "\n",
    "# fields =  ['Dichtlacker', 'Heindlacker', 'Heng',\n",
    "#                            'Holzacker', 'Neulandsiedlung','Itzling5',\n",
    "#                            'Itzling6', 'Schluetterfabrik', 'Thalhausen138', 'Voettingerfeld']\n",
    "\n",
    "test_fields = ['Baumacker', 'Itzling2', 'Thalhausen141']\n",
    "\n",
    "field_summary = pd.read_excel(\n",
    "    \"../../data/cropdata/Bavaria/yields/fields_summary.xlsx\")\n",
    "yields_2018 = pd.read_csv(\"../../data/cropdata/Bavaria/yields/yields2018.csv\")\n",
    "yields_df = yields_2018.copy()\n",
    "yields_df2 = yields_2018.copy()\n",
    "\n",
    "bands = [\"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"]\n",
    "angles = ['solar_zenith', 'observer_zenith', 'relative_azimuth']\n",
    "other_features = [\"et0\", \"rain\", \"cum_rain\"]\n",
    "feature_cols = bands + other_features\n",
    "target_col = \"Ertr.masse (Nass)(tonne/ha)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = yields_df.Name.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion = 1\n",
    "\n",
    "\n",
    "def getYieldwithoutBorders(group):\n",
    "    # print(group['Name'].values[0])\n",
    "    _fieldname = group['Name'].values[0]\n",
    "    geo_df = gpd.GeoDataFrame.from_file(\n",
    "        '../../data/cropdata/Bavaria/yields/FeldstueckeTUM/Feldstuecke_WGS84.shp'\n",
    "    )\n",
    "    geo_df = geo_df[geo_df.Name_new == _fieldname]\n",
    "    geo_df2 = geo_df.buffer(-0.00004, resolution=1)\n",
    "    # put Lon, Lat from dataframe to GeoDataFrame\n",
    "    geometry = [Point(xy) for xy in zip(group.Longitude, group.Latitude)]\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "    # schneide group mit felddaten\n",
    "    gdf = gpd.GeoDataFrame(group, crs=crs, geometry=geometry)\n",
    "    mask = gdf.geometry.within(geo_df2.geometry.unary_union)\n",
    "    newdata = gdf.loc[mask]\n",
    "    # ertrag cut als einzelwert f√ºrs feld schreiben\n",
    "    group['Ertrag_wBorders'] = newdata['Ertr.masse (Nass)(tonne/ha)'].sum(\n",
    "    ) * conversion / newdata['Ertr.masse (Nass)(tonne/ha)'].shape[0]\n",
    "    return group\n",
    "\n",
    "\n",
    "yields_df = yields_df.groupby(['Name']).apply(getYieldwithoutBorders)\n",
    "yields_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions from https://github.com/ADA-research/AutoML4HybridEarthScienceModels\n",
    "\n",
    "\n",
    "def extract_date_from_url(url):\n",
    "    idx = url.find(\"TIME=\")\n",
    "    return url[idx + 5:idx + 15]\n",
    "\n",
    "\n",
    "def map_to_degrees(x):\n",
    "    if x < 0:\n",
    "        x = 360 + x\n",
    "    else:\n",
    "        x = x\n",
    "    return x\n",
    "\n",
    "\n",
    "def filter_by_2std(mean, std, target, data):\n",
    "    condition = mean + 2 * std\n",
    "    condition2 = mean - 2 * std\n",
    "    return data[(data[target] < condition) & (data[target] > condition2)]\n",
    "\n",
    "\n",
    "def drop_unnamed_columns(df):\n",
    "    \"\"\"\n",
    "    When saving/loading .csv files, the index is often saved as an unnamed column.\n",
    "    This function removes any unnamed columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd DataFrame): input DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    return df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "\n",
    "def create_pixelwise_S2_data(yields_df, fields, path):\n",
    "    s2_cols = [\n",
    "        \"CLM\", \"dataMask\", \"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\",\n",
    "        \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"solar_azimuth\", \"solar_zenith\",\n",
    "        \"observer_azimuth\", \"observer_zenith\", \"unknown\"\n",
    "    ]\n",
    "\n",
    "    data = []\n",
    "    for field in tqdm(fields):\n",
    "        yield_data = yields_df[yields_df[\"Name\"] == field][[\n",
    "            \"Latitude\", \"Longitude\", \"Ertr.masse (Nass)(tonne/ha)\",\n",
    "            \"ErtragNass\"\n",
    "        ]]\n",
    "\n",
    "        for img_dir in os.listdir(os.path.join(path, field)):\n",
    "            # Read satellite image with rasterio\n",
    "            src = rio.open(os.path.join(path, field, img_dir, \"response.tiff\"),\n",
    "                           mode=\"r+\")\n",
    "            # Extract image time from json request\n",
    "            msg = json.loads(\n",
    "                open(os.path.join(path, field, img_dir,\n",
    "                                  \"request.json\")).read())\n",
    "            img_date = extract_date_from_url(msg[\"url\"])\n",
    "\n",
    "            # Get reflectance values per pixel\n",
    "            transformer = Transformer.from_crs(\"EPSG:4326\",\n",
    "                                               \"EPSG:3857\",\n",
    "                                               authority=\"EPSG\")\n",
    "            yield_data[\"x\"], yield_data[\"y\"] = transformer.transform(\n",
    "                yield_data[\"Latitude\"], yield_data[\"Longitude\"])\n",
    "\n",
    "            s2_data = list(\n",
    "                rio.sample.sample_gen(src, yield_data[[\"x\", \"y\"]].values))\n",
    "            try:\n",
    "                temp_df = pd.DataFrame(s2_data,\n",
    "                                       columns=s2_cols).drop_duplicates().join(\n",
    "                                           yield_data.reset_index())\n",
    "\n",
    "                temp_df[\"relative_azimuth\"] = (temp_df[\"solar_azimuth\"] - temp_df[\"observer_azimuth\"])\\\n",
    "                    .apply(map_to_degrees)\n",
    "\n",
    "                temp_df[\"date\"] = img_date\n",
    "                temp_df[\"Name\"] = field\n",
    "                data.append(temp_df)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Failed to extract reflectance values from: {}\".format(\n",
    "                    os.path.join(path, field, img_dir, \"response.tiff\")))\n",
    "\n",
    "    data = pd.concat(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def resample_and_merge_data(sat_df, et0_df, frequency=\"W\"):\n",
    "    \"\"\"\n",
    "    Creates a weekly or monthly resampled dataset from satellite data and rain/et0 data\n",
    "\n",
    "    Args:\n",
    "        sat_df (pd DataFrame): S2A reflectance data\n",
    "        et0_df (pd DataFrame): rain/et0 data\n",
    "        frequency (str): \"W\" for weekly or \"M\" for monthly\n",
    "    \"\"\"\n",
    "\n",
    "    sat_df[\"date\"] = pd.to_datetime(sat_df[\"date\"])\n",
    "    et0_df[\"date\"] = pd.to_datetime(et0_df[\"date\"])\n",
    "\n",
    "    # Filter by cloud mask\n",
    "    sat_df = sat_df[sat_df[\"CLM\"] == 0]\n",
    "\n",
    "    # Resample reflectance data to frequency\n",
    "    sat_df = sat_df.groupby(\"index\").resample(frequency,\n",
    "                                              on=\"date\").mean().interpolate()\n",
    "    sat_df = sat_df.reset_index(\"date\")\n",
    "\n",
    "    et0_df = et0_df[[\"date\", \"et0\", \"rain\", \"cum_rain\"]].drop_duplicates()\n",
    "\n",
    "    # Resample et0 data to frequency, starting at the same date as sat_df\n",
    "    # Maybe it would be better to use a sum/mean over time for et0 and rain instead of resampling\n",
    "    et0_df = et0_df.resample(frequency, on=\"date\",\n",
    "                             origin=sat_df[\"date\"].min()).mean().interpolate()\n",
    "    et0_df = et0_df.reset_index(\"date\")\n",
    "\n",
    "    df = sat_df.merge(et0_df, left_on=\"date\", right_on=\"date\")\n",
    "    df = drop_unnamed_columns(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def invert_rtm(rtm_df,\n",
    "               model,\n",
    "               hyperparams,\n",
    "               feature_cols,\n",
    "               target_col=\"lai\",\n",
    "               do_cv=True):\n",
    "\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                         ('model', model(**hyperparams))])\n",
    "    # Normally you would fit hyperparameters separately,\n",
    "    # for now just show cv score here to get an idea of inversion performance\n",
    "    if do_cv:\n",
    "        results = cross_validate(pipeline,\n",
    "                                 X=rtm_df[feature_cols],\n",
    "                                 y=rtm_df[target_col],\n",
    "                                 cv=5,\n",
    "                                 scoring=('r2', 'neg_mean_squared_error'),\n",
    "                                 return_train_score=True)\n",
    "\n",
    "        display(\"Inversion for {}\".format(target_col))\n",
    "        display(\"Mean train R2: {}, individual folds: {}\".format(\n",
    "            np.mean(results[\"train_r2\"]), results[\"train_r2\"]))\n",
    "        display(\"Mean test R2: {}, individual folds: {}\\n\".format(\n",
    "            np.mean(results[\"test_r2\"]), results[\"test_r2\"]))\n",
    "\n",
    "    pipeline.fit(rtm_df[feature_cols], rtm_df[target_col])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def create_dataset(bands,\n",
    "                   yields_df,\n",
    "                   fields,\n",
    "                   should_create_files=True,\n",
    "                   include_rtm=False,\n",
    "                   frequency=\"W\"):\n",
    "    data_path = \".\"\n",
    "    if should_create_files:\n",
    "        # To create locally:\n",
    "\n",
    "        sat_images_path = \"../../data/cropdata/Bavaria/yields/sat_images_10m/\"\n",
    "\n",
    "        # yields_df = pd.read_csv(os.path.join(data_path, \"../datayields2018.csv\"))\n",
    "        fields_of_interest = fields\n",
    "\n",
    "        sat_df = create_pixelwise_S2_data(yields_df, fields_of_interest,\n",
    "                                          sat_images_path)\n",
    "        # S2 values are scaled by a factor 10000\n",
    "        sat_df[bands] = sat_df[bands] / 10000\n",
    "        et0_df = pd.read_excel(\n",
    "            os.path.join(\n",
    "                \"../../data/cropdata/Bavaria/yields/satellite_data_orginal.xlsx\"\n",
    "            ))\n",
    "\n",
    "        df = resample_and_merge_data(sat_df, et0_df, frequency)\n",
    "\n",
    "    else:\n",
    "        # To simply load files that were already created:\n",
    "        filename = \"reflectance_per_pixel_weekly_10m_rtm.csv\" \\\n",
    "            if frequency == \"W\" else \"reflectance_per_pixel_monthly_10m_rtm.csv\"\n",
    "        df = pd.read_csv(os.path.join(data_path, filename))\n",
    "\n",
    "    if include_rtm:\n",
    "        # For now use a similar simple model setup for RTM inversion\n",
    "        rf = RandomForestRegressor\n",
    "        hyperparams = {\n",
    "            \"n_jobs\": -1,\n",
    "            \"n_estimators\": 300,\n",
    "            \"max_depth\": 100,\n",
    "            \"max_features\": 'sqrt',\n",
    "            \"random_state\": 984\n",
    "        }\n",
    "\n",
    "        include_angles = True\n",
    "\n",
    "        angles = ['solar_zenith', 'observer_zenith', 'relative_azimuth']\n",
    "        features = bands + angles if include_angles else bands\n",
    "\n",
    "        lai_model = invert_rtm(rtm_df,\n",
    "                               rf,\n",
    "                               hyperparams,\n",
    "                               feature_cols=features,\n",
    "                               target_col=\"lai\")\n",
    "        cm_model = invert_rtm(rtm_df,\n",
    "                              rf,\n",
    "                              hyperparams,\n",
    "                              feature_cols=features,\n",
    "                              target_col=\"cm\")\n",
    "        cab_model = invert_rtm(rtm_df,\n",
    "                               rf,\n",
    "                               hyperparams,\n",
    "                               feature_cols=features,\n",
    "                               target_col=\"cab\")\n",
    "\n",
    "        df[\"lai\"] = lai_model.predict(df[features])\n",
    "        df[\"cm\"] = cm_model.predict(df[features])\n",
    "        df[\"cab\"] = cab_model.predict(df[features])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def flatten_time_series(df, feature_cols, target_col):\n",
    "    \"\"\"\n",
    "    Flattens a dataset for use in a supervised model. Not suitable for recurrent models.\n",
    "\n",
    "    Args:\n",
    "        df (pd DataFrame):\n",
    "        feature_cols (list of str): Feature column names\n",
    "        target_col (str):\n",
    "\n",
    "    return\n",
    "        df (pd DataFrame):\n",
    "        feature_cols (list of str): New feature column names with \n",
    "                                    suffix for each timestep, \n",
    "                                    e.g. _t-5 for 5 weeks/months before last timestep\n",
    "    \"\"\"\n",
    "\n",
    "    out_df = []\n",
    "    for field_index in df[\"index\"].unique():\n",
    "        sub_df = df[df[\"index\"] == field_index]\n",
    "        n_timesteps = len(sub_df)\n",
    "        cols = list(\n",
    "            np.array([[col + \"_t-{}\".format(i) for col in feature_cols]\n",
    "                      for i in reversed(range(n_timesteps))]).flatten())\n",
    "        ts_df = pd.DataFrame(sub_df[feature_cols].values.flatten()).T\n",
    "        ts_df.columns = cols\n",
    "        ts_df[target_col] = sub_df.iloc[0][target_col]\n",
    "        out_df.append(ts_df)\n",
    "    return pd.concat(out_df).interpolate(), cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myields_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myields_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m out_df, feature_cols \u001b[38;5;241m=\u001b[39m flatten_time_series(df, feature_cols,\n\u001b[1;32m      5\u001b[0m                                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErtr.masse (Nass)(tonne/ha)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m mean \u001b[38;5;241m=\u001b[39m out_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErtr.masse (Nass)(tonne/ha)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[11], line 168\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(bands, yields_df, fields, should_create_files, include_rtm, frequency)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# yields_df = pd.read_csv(os.path.join(data_path, \"../datayields2018.csv\"))\u001b[39;00m\n\u001b[1;32m    166\u001b[0m fields_of_interest \u001b[38;5;241m=\u001b[39m fields\n\u001b[0;32m--> 168\u001b[0m sat_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_pixelwise_S2_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43myields_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields_of_interest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43msat_images_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# S2 values are scaled by a factor 10000\u001b[39;00m\n\u001b[1;32m    171\u001b[0m sat_df[bands] \u001b[38;5;241m=\u001b[39m sat_df[bands] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10000\u001b[39m\n",
      "Cell \u001b[0;32mIn[11], line 66\u001b[0m, in \u001b[0;36mcreate_pixelwise_S2_data\u001b[0;34m(yields_df, fields, path)\u001b[0m\n\u001b[1;32m     60\u001b[0m transformer \u001b[38;5;241m=\u001b[39m Transformer\u001b[38;5;241m.\u001b[39mfrom_crs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG:4326\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     61\u001b[0m                                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG:3857\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     62\u001b[0m                                    authority\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m yield_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m], yield_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m     64\u001b[0m     yield_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m], yield_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 66\u001b[0m s2_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myield_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     temp_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(s2_data,\n\u001b[1;32m     70\u001b[0m                            columns\u001b[38;5;241m=\u001b[39ms2_cols)\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     71\u001b[0m                                yield_data\u001b[38;5;241m.\u001b[39mreset_index())\n",
      "File \u001b[0;32m~/Dev/WorldCrops/env/lib/python3.10/site-packages/rasterio/sample.py:61\u001b[0m, in \u001b[0;36msample_gen\u001b[0;34m(dataset, xy, indexes, masked)\u001b[0m\n\u001b[1;32m     58\u001b[0m     nodata \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39marray(nodata, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pts \u001b[38;5;129;01min\u001b[39;00m _grouper(xy, \u001b[38;5;241m256\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m     pts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row_off, col_off \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mrowcol(dt, \u001b[38;5;241m*\u001b[39mpts)):\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m row_off \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m col_off \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m row_off \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m height \u001b[38;5;129;01mor\u001b[39;00m col_off \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m width:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "df = create_dataset(bands=bands, yields_df=yields_df, fields=fields)\n",
    "out_df, feature_cols = flatten_time_series(df, feature_cols,\n",
    "                                           \"Ertr.masse (Nass)(tonne/ha)\")\n",
    "\n",
    "mean = out_df['Ertr.masse (Nass)(tonne/ha)'].mean()\n",
    "std = out_df['Ertr.masse (Nass)(tonne/ha)'].std()\n",
    "\n",
    "# out_df = filter_by_2std(mean, std,'Ertr.masse (Nass)(tonne/ha)', out_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60337cd91834e8cb14570951b1878da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n"
     ]
    }
   ],
   "source": [
    "df_test = create_dataset(bands=bands, yields_df=yields_df2, fields=test_fields)\n",
    "out_df_test, feature_cols = flatten_time_series(df_test, feature_cols,\n",
    "                                                \"Ertr.masse (Nass)(tonne/ha)\")\n",
    "\n",
    "mean = out_df['Ertr.masse (Nass)(tonne/ha)'].mean()\n",
    "std = out_df['Ertr.masse (Nass)(tonne/ha)'].std()\n",
    "\n",
    "out_df_test = filter_by_2std(mean, std, 'Ertr.masse (Nass)(tonne/ha)',\n",
    "                             out_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df[feature_cols]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(out_df[feature_cols].values)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(out_df[feature_cols].values)\n",
    "x_scaled2 = scaler.transform(out_df[feature_cols].values)\n",
    "\n",
    "test = pd.DataFrame(columns=feature_cols, data=x_scaled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = filter_by_2std(mean, std, 'Ertr.masse (Nass)(tonne/ha)', out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mean train R2: 0.9432445524598606, individual folds: [0.94179109 0.94336174 0.94346173 0.94375781 0.94385039]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Mean test R2: 0.582317949755853, individual folds: [0.58620178 0.58690591 0.58051324 0.57301315 0.58495566]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train RF with pixels and apply\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs=4, n_estimators=100)\n",
    "\n",
    "target_col = \"Ertr.masse (Nass)(tonne/ha)\"\n",
    "# feature_cols = list(range(0, 326))\n",
    "\n",
    "results = cross_validate(rf,\n",
    "                         X=out_df[feature_cols],\n",
    "                         y=out_df[target_col],\n",
    "                         cv=cv,\n",
    "                         scoring=('r2', 'neg_mean_squared_error'),\n",
    "                         return_train_score=True)\n",
    "\n",
    "display(\"Mean train R2: {}, individual folds: {}\".format(\n",
    "    np.mean(results[\"train_r2\"]), results[\"train_r2\"]))\n",
    "display(\"Mean test R2: {}, individual folds: {}\".format(\n",
    "    np.mean(results[\"test_r2\"]), results[\"test_r2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.2612001401996652\n",
      "Accuracy of classifier Cross Validation: 0.53\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 300\n",
    "max_depth = 100\n",
    "max_features = 'sqrt'\n",
    "J = 984\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                            max_depth=max_depth,\n",
    "                            max_features=max_features,\n",
    "                            random_state=J)  # Train the model on training data\n",
    "clf.fit(out_df[feature_cols], out_df[target_col])\n",
    "\n",
    "predictions = clf.predict(out_df_test[feature_cols])\n",
    "print(\"R2:\", r2_score(out_df_test[target_col], predictions))\n",
    "\n",
    "score = cross_val_score(clf,\n",
    "                        X=out_df_test[feature_cols],\n",
    "                        y=out_df_test[target_col],\n",
    "                        cv=cv)\n",
    "print('Accuracy of classifier Cross Validation: {:.2f}'.format(score.mean()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "754299ca8a2537c9a9f489998c42930686a67643463836b55bb352a8eff7971c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
