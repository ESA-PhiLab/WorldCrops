{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo for visualisation of crop type and yield data\n",
    "import warnings\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "import rasterio as rio\n",
    "import os\n",
    "\n",
    "# 3D stuff\n",
    "from IPython.core.display import display\n",
    "import json\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "from shapely.geometry import Point  # Point class\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    'Baumacker', 'D8', 'Dichtlacker', 'Heindlacker', 'Heng', 'Holzacker',\n",
    "    'Neulandsiedlung', 'Itzling2', 'Itzling5', 'Itzling6', 'Schluetterfabrik',\n",
    "    'Thalhausen138', 'Thalhausen141', 'Voettingerfeld'\n",
    "]\n",
    "\n",
    "# fields =  ['Dichtlacker', 'Heindlacker', 'Heng',\n",
    "#                            'Holzacker', 'Neulandsiedlung','Itzling5',\n",
    "#                            'Itzling6', 'Schluetterfabrik', 'Thalhausen138', 'Voettingerfeld']\n",
    "\n",
    "test_fields = ['Baumacker', 'Itzling2', 'Thalhausen141']\n",
    "\n",
    "field_summary = pd.read_excel(\n",
    "    \"../../data/cropdata/Bavaria/yields/fields_summary.xlsx\")\n",
    "yields_2018 = pd.read_csv(\"../../data/cropdata/Bavaria/yields/yields2018.csv\")\n",
    "yields_df = yields_2018.copy()\n",
    "yields_df2 = yields_2018.copy()\n",
    "\n",
    "bands = [\"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"]\n",
    "angles = ['solar_zenith', 'observer_zenith', 'relative_azimuth']\n",
    "other_features = [\"et0\", \"rain\", \"cum_rain\"]\n",
    "feature_cols = bands + other_features\n",
    "target_col = \"Ertr.masse (Nass)(tonne/ha)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = yields_df.Name.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion = 1\n",
    "\n",
    "\n",
    "def getYieldwithoutBorders(group):\n",
    "    # print(group['Name'].values[0])\n",
    "    _fieldname = group['Name'].values[0]\n",
    "    geo_df = gpd.GeoDataFrame.from_file(\n",
    "        '../../data/cropdata/Bavaria/yields/FeldstueckeTUM/Feldstuecke_WGS84.shp'\n",
    "    )\n",
    "    geo_df = geo_df[geo_df.Name_new == _fieldname]\n",
    "    geo_df2 = geo_df.buffer(-0.00004, resolution=1)\n",
    "    # put Lon, Lat from dataframe to GeoDataFrame\n",
    "    geometry = [Point(xy) for xy in zip(group.Longitude, group.Latitude)]\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "    # schneide group mit felddaten\n",
    "    gdf = gpd.GeoDataFrame(group, crs=crs, geometry=geometry)\n",
    "    mask = gdf.geometry.within(geo_df2.geometry.unary_union)\n",
    "    newdata = gdf.loc[mask]\n",
    "    # ertrag cut als einzelwert fürs feld schreiben\n",
    "    group['Ertrag_wBorders'] = newdata['Ertr.masse (Nass)(tonne/ha)'].sum(\n",
    "    ) * conversion / newdata['Ertr.masse (Nass)(tonne/ha)'].shape[0]\n",
    "    return group\n",
    "\n",
    "\n",
    "yields_df = yields_df.groupby(['Name']).apply(getYieldwithoutBorders)\n",
    "yields_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions from https://github.com/ADA-research/AutoML4HybridEarthScienceModels\n",
    "def extract_date_from_url(url):\n",
    "    return url[url.find(\"TIME=\") + 5:url.find(\"TIME=\") + 15]\n",
    "\n",
    "\n",
    "def map_to_degrees(x):\n",
    "    return x + 360 if x < 0 else x\n",
    "\n",
    "\n",
    "def filter_by_2std(mean, std, target, data):\n",
    "    upper_limit = mean + 2 * std\n",
    "    lower_limit = mean - 2 * std\n",
    "    return data[(data[target] < upper_limit) & (data[target] > lower_limit)]\n",
    "\n",
    "\n",
    "def drop_unnamed_columns(df):\n",
    "    return df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "\n",
    "def create_pixelwise_S2_data(yields_df, fields, path):\n",
    "    s2_cols = [\"CLM\", \"dataMask\", \"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\",\n",
    "               \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"solar_azimuth\", \"solar_zenith\",\n",
    "               \"observer_azimuth\", \"observer_zenith\", \"unknown\"]\n",
    "\n",
    "    data = []\n",
    "    for field in tqdm(fields):\n",
    "        yield_data = yields_df[yields_df[\"Name\"] == field][\n",
    "            [\"Latitude\", \"Longitude\", \"Ertr.masse (Nass)(tonne/ha)\", \"ErtragNass\"]]\n",
    "\n",
    "        for img_dir in os.listdir(os.path.join(path, field)):\n",
    "            try:\n",
    "                with rio.open(os.path.join(path, field, img_dir, \"response.tiff\"), mode=\"r+\") as src, \\\n",
    "                        open(os.path.join(path, field, img_dir, \"request.json\")) as req_file:\n",
    "                    msg = json.load(req_file)\n",
    "                    img_date = extract_date_from_url(msg[\"url\"])\n",
    "\n",
    "                    transformer = Transformer.from_crs(\n",
    "                        \"EPSG:4326\", \"EPSG:3857\", authority=\"EPSG\")\n",
    "                    yield_data[\"x\"], yield_data[\"y\"] = transformer.transform(\n",
    "                        yield_data[\"Latitude\"], yield_data[\"Longitude\"])\n",
    "\n",
    "                    s2_data = list(rio.sample.sample_gen(\n",
    "                        src, yield_data[[\"x\", \"y\"]].values))\n",
    "                    temp_df = pd.DataFrame(s2_data, columns=s2_cols).drop_duplicates().join(\n",
    "                        yield_data.reset_index())\n",
    "                    temp_df[\"relative_azimuth\"] = (\n",
    "                        temp_df[\"solar_azimuth\"] - temp_df[\"observer_azimuth\"]).apply(map_to_degrees)\n",
    "                    temp_df[\"date\"] = img_date\n",
    "                    temp_df[\"Name\"] = field\n",
    "                    data.append(temp_df)\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Error processing {os.path.join(path, field, img_dir, 'response.tiff')}: {e}\")\n",
    "\n",
    "    return pd.concat(data)\n",
    "\n",
    "\n",
    "def resample_and_merge_data(sat_df, et0_df, frequency=\"W\"):\n",
    "    sat_df[\"date\"] = pd.to_datetime(sat_df[\"date\"])\n",
    "    et0_df[\"date\"] = pd.to_datetime(et0_df[\"date\"])\n",
    "\n",
    "    sat_df = sat_df[sat_df[\"CLM\"] == 0]\n",
    "    sat_df = sat_df.groupby(\"index\").resample(\n",
    "        frequency, on=\"date\").mean().interpolate().reset_index(\"date\")\n",
    "    et0_df = et0_df[[\"date\", \"et0\", \"rain\", \"cum_rain\"]].drop_duplicates()\n",
    "    et0_df = et0_df.resample(frequency, on=\"date\", origin=sat_df[\"date\"].min(\n",
    "    )).mean().interpolate().reset_index(\"date\")\n",
    "\n",
    "    return drop_unnamed_columns(sat_df.merge(et0_df, on=\"date\"))\n",
    "\n",
    "\n",
    "def invert_rtm(rtm_df, model, hyperparams, feature_cols, target_col=\"lai\", do_cv=True):\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                        ('model', model(**hyperparams))])\n",
    "    if do_cv:\n",
    "        results = cross_validate(pipeline, X=rtm_df[feature_cols], y=rtm_df[target_col],\n",
    "                                 cv=5, scoring=('r2', 'neg_mean_squared_error'), return_train_score=True)\n",
    "        print(f\"Inversion for {target_col}\")\n",
    "        print(\n",
    "            f\"Mean train R2: {np.mean(results['train_r2'])}, individual folds: {results['train_r2']}\")\n",
    "        print(\n",
    "            f\"Mean test R2: {np.mean(results['test_r2'])}, individual folds: {results['test_r2']}\\n\")\n",
    "\n",
    "    pipeline.fit(rtm_df[feature_cols], rtm_df[target_col])\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def create_dataset(bands,\n",
    "                   yields_df,\n",
    "                   fields,\n",
    "                   should_create_files=True,\n",
    "                   include_rtm=False,\n",
    "                   frequency=\"W\"):\n",
    "    data_path = \".\"\n",
    "    if should_create_files:\n",
    "        # To create locally:\n",
    "\n",
    "        sat_images_path = \"../../data/cropdata/Bavaria/yields/sat_images_10m/\"\n",
    "\n",
    "        # yields_df = pd.read_csv(os.path.join(data_path, \"../datayields2018.csv\"))\n",
    "        fields_of_interest = fields\n",
    "\n",
    "        sat_df = create_pixelwise_S2_data(yields_df, fields_of_interest,\n",
    "                                          sat_images_path)\n",
    "        # S2 values are scaled by a factor 10000\n",
    "        sat_df[bands] = sat_df[bands] / 10000\n",
    "        et0_df = pd.read_excel(\n",
    "            os.path.join(\n",
    "                \"../../data/cropdata/Bavaria/yields/satellite_data_orginal.xlsx\"\n",
    "            ))\n",
    "\n",
    "        df = resample_and_merge_data(sat_df, et0_df, frequency)\n",
    "\n",
    "    else:\n",
    "        # To simply load files that were already created:\n",
    "        filename = \"reflectance_per_pixel_weekly_10m_rtm.csv\" \\\n",
    "            if frequency == \"W\" else \"reflectance_per_pixel_monthly_10m_rtm.csv\"\n",
    "        df = pd.read_csv(os.path.join(data_path, filename))\n",
    "\n",
    "    if include_rtm:\n",
    "        # For now use a similar simple model setup for RTM inversion\n",
    "        rf = RandomForestRegressor\n",
    "        hyperparams = {\n",
    "            \"n_jobs\": -1,\n",
    "            \"n_estimators\": 300,\n",
    "            \"max_depth\": 100,\n",
    "            \"max_features\": 'sqrt',\n",
    "            \"random_state\": 984\n",
    "        }\n",
    "\n",
    "        include_angles = True\n",
    "\n",
    "        angles = ['solar_zenith', 'observer_zenith', 'relative_azimuth']\n",
    "        features = bands + angles if include_angles else bands\n",
    "\n",
    "        lai_model = invert_rtm(rtm_df,\n",
    "                               rf,\n",
    "                               hyperparams,\n",
    "                               feature_cols=features,\n",
    "                               target_col=\"lai\")\n",
    "        cm_model = invert_rtm(rtm_df,\n",
    "                              rf,\n",
    "                              hyperparams,\n",
    "                              feature_cols=features,\n",
    "                              target_col=\"cm\")\n",
    "        cab_model = invert_rtm(rtm_df,\n",
    "                               rf,\n",
    "                               hyperparams,\n",
    "                               feature_cols=features,\n",
    "                               target_col=\"cab\")\n",
    "\n",
    "        df[\"lai\"] = lai_model.predict(df[features])\n",
    "        df[\"cm\"] = cm_model.predict(df[features])\n",
    "        df[\"cab\"] = cab_model.predict(df[features])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def flatten_time_series(df, feature_cols, target_col):\n",
    "    \"\"\"\n",
    "    Flattens a dataset for use in a supervised model. Not suitable for recurrent models.\n",
    "\n",
    "    Args:\n",
    "        df (pd DataFrame):\n",
    "        feature_cols (list of str): Feature column names\n",
    "        target_col (str):\n",
    "\n",
    "    return\n",
    "        df (pd DataFrame):\n",
    "        feature_cols (list of str): New feature column names with \n",
    "                                    suffix for each timestep, \n",
    "                                    e.g. _t-5 for 5 weeks/months before last timestep\n",
    "    \"\"\"\n",
    "\n",
    "    out_df = []\n",
    "    for field_index in df[\"index\"].unique():\n",
    "        sub_df = df[df[\"index\"] == field_index]\n",
    "        n_timesteps = len(sub_df)\n",
    "        cols = list(\n",
    "            np.array([[col + \"_t-{}\".format(i) for col in feature_cols]\n",
    "                      for i in reversed(range(n_timesteps))]).flatten())\n",
    "        ts_df = pd.DataFrame(sub_df[feature_cols].values.flatten()).T\n",
    "        ts_df.columns = cols\n",
    "        ts_df[target_col] = sub_df.iloc[0][target_col]\n",
    "        out_df.append(ts_df)\n",
    "    return pd.concat(out_df).interpolate(), cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_dataset(bands,\n",
    "                   yields_df,\n",
    "                   fields,\n",
    "                   should_create_files=True,\n",
    "                   include_rtm=False,\n",
    "                   frequency=\"W\"):\n",
    "    data_path = \".\"\n",
    "    if should_create_files:\n",
    "        # To create locally:\n",
    "\n",
    "        sat_images_path = \"../../data/cropdata/Bavaria/yields/sat_images_10m/\"\n",
    "\n",
    "        # yields_df = pd.read_csv(os.path.join(data_path, \"../datayields2018.csv\"))\n",
    "        fields_of_interest = fields\n",
    "\n",
    "        sat_df = create_pixelwise_S2_data(yields_df, fields_of_interest,\n",
    "                                          sat_images_path)\n",
    "        # S2 values are scaled by a factor 10000\n",
    "        sat_df[bands] = sat_df[bands] / 10000\n",
    "        et0_df = pd.read_excel(\n",
    "            os.path.join(\n",
    "                \"../../data/cropdata/Bavaria/yields/satellite_data_orginal.xlsx\"\n",
    "            ))\n",
    "\n",
    "        df = resample_and_merge_data(sat_df, et0_df, frequency)\n",
    "\n",
    "    else:\n",
    "        # To simply load files that were already created:\n",
    "        filename = \"reflectance_per_pixel_weekly_10m_rtm.csv\" \\\n",
    "            if frequency == \"W\" else \"reflectance_per_pixel_monthly_10m_rtm.csv\"\n",
    "        df = pd.read_csv(os.path.join(data_path, filename))\n",
    "\n",
    "    if include_rtm:\n",
    "        # For now use a similar simple model setup for RTM inversion\n",
    "        rf = RandomForestRegressor\n",
    "        hyperparams = {\n",
    "            \"n_jobs\": -1,\n",
    "            \"n_estimators\": 300,\n",
    "            \"max_depth\": 100,\n",
    "            \"max_features\": 'sqrt',\n",
    "            \"random_state\": 984\n",
    "        }\n",
    "\n",
    "        include_angles = True\n",
    "\n",
    "        angles = ['solar_zenith', 'observer_zenith', 'relative_azimuth']\n",
    "        features = bands + angles if include_angles else bands\n",
    "\n",
    "        lai_model = invert_rtm(rtm_df,\n",
    "                               rf,\n",
    "                               hyperparams,\n",
    "                               feature_cols=features,\n",
    "                               target_col=\"lai\")\n",
    "        cm_model = invert_rtm(rtm_df,\n",
    "                              rf,\n",
    "                              hyperparams,\n",
    "                              feature_cols=features,\n",
    "                              target_col=\"cm\")\n",
    "        cab_model = invert_rtm(rtm_df,\n",
    "                               rf,\n",
    "                               hyperparams,\n",
    "                               feature_cols=features,\n",
    "                               target_col=\"cab\")\n",
    "\n",
    "        df[\"lai\"] = lai_model.predict(df[features])\n",
    "        df[\"cm\"] = cm_model.predict(df[features])\n",
    "        df[\"cab\"] = cab_model.predict(df[features])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [02:53<00:00,  5.58s/it]\n"
     ]
    }
   ],
   "source": [
    "df = create_dataset(bands=bands, yields_df=yields_df, fields=fields)\n",
    "out_df, feature_cols = flatten_time_series(df, feature_cols,\n",
    "                                           \"Ertr.masse (Nass)(tonne/ha)\")\n",
    "\n",
    "mean = out_df['Ertr.masse (Nass)(tonne/ha)'].mean()\n",
    "std = out_df['Ertr.masse (Nass)(tonne/ha)'].std()\n",
    "\n",
    "# out_df = filter_by_2std(mean, std,'Ertr.masse (Nass)(tonne/ha)', out_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B04_t-20</th>\n",
       "      <th>B05_t-20</th>\n",
       "      <th>B06_t-20</th>\n",
       "      <th>B07_t-20</th>\n",
       "      <th>B08_t-20</th>\n",
       "      <th>B8A_t-20</th>\n",
       "      <th>B09_t-20</th>\n",
       "      <th>B11_t-20</th>\n",
       "      <th>B12_t-20</th>\n",
       "      <th>et0_t-20</th>\n",
       "      <th>...</th>\n",
       "      <th>B07_t-0</th>\n",
       "      <th>B08_t-0</th>\n",
       "      <th>B8A_t-0</th>\n",
       "      <th>B09_t-0</th>\n",
       "      <th>B11_t-0</th>\n",
       "      <th>B12_t-0</th>\n",
       "      <th>et0_t-0</th>\n",
       "      <th>rain_t-0</th>\n",
       "      <th>cum_rain_t-0</th>\n",
       "      <th>Ertr.masse (Nass)(tonne/ha)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>12340.000000</td>\n",
       "      <td>1.234000e+04</td>\n",
       "      <td>1.234000e+04</td>\n",
       "      <td>12340.000</td>\n",
       "      <td>12340.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1.298619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>4.420460e+00</td>\n",
       "      <td>3.440000e-01</td>\n",
       "      <td>321.342</td>\n",
       "      <td>6.642245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>8.882144e-16</td>\n",
       "      <td>5.551340e-17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.099927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.298619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.420460e+00</td>\n",
       "      <td>3.440000e-01</td>\n",
       "      <td>321.342</td>\n",
       "      <td>0.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.298619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>4.420460e+00</td>\n",
       "      <td>3.440000e-01</td>\n",
       "      <td>321.342</td>\n",
       "      <td>5.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>1.298619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>4.420460e+00</td>\n",
       "      <td>3.440000e-01</td>\n",
       "      <td>321.342</td>\n",
       "      <td>6.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.298619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>4.420460e+00</td>\n",
       "      <td>3.440000e-01</td>\n",
       "      <td>321.342</td>\n",
       "      <td>8.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>1.298619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>4.420460e+00</td>\n",
       "      <td>3.440000e-01</td>\n",
       "      <td>321.342</td>\n",
       "      <td>17.240000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 253 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           B04_t-20      B05_t-20      B06_t-20      B07_t-20      B08_t-20  \\\n",
       "count  12340.000000  12340.000000  12340.000000  12340.000000  12340.000000   \n",
       "mean       0.000008      0.000013      0.000018      0.000020      0.000021   \n",
       "std        0.000003      0.000004      0.000007      0.000008      0.000008   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000007      0.000011      0.000015      0.000016      0.000017   \n",
       "50%        0.000009      0.000013      0.000018      0.000019      0.000021   \n",
       "75%        0.000010      0.000015      0.000022      0.000024      0.000025   \n",
       "max        0.000047      0.000039      0.000045      0.000048      0.000052   \n",
       "\n",
       "           B8A_t-20      B09_t-20      B11_t-20      B12_t-20      et0_t-20  \\\n",
       "count  12340.000000  12340.000000  12340.000000  12340.000000  12340.000000   \n",
       "mean       0.000021      0.000021      0.000017      0.000012      1.298619   \n",
       "std        0.000008      0.000008      0.000006      0.000004      0.000000   \n",
       "min        0.000000      0.000000      0.000000      0.000000      1.298619   \n",
       "25%        0.000017      0.000018      0.000016      0.000011      1.298619   \n",
       "50%        0.000020      0.000021      0.000018      0.000013      1.298619   \n",
       "75%        0.000025      0.000026      0.000020      0.000014      1.298619   \n",
       "max        0.000050      0.000046      0.000034      0.000029      1.298619   \n",
       "\n",
       "       ...       B07_t-0       B08_t-0       B8A_t-0       B09_t-0  \\\n",
       "count  ...  12340.000000  12340.000000  12340.000000  12340.000000   \n",
       "mean   ...      0.000020      0.000020      0.000022      0.000023   \n",
       "std    ...      0.000006      0.000006      0.000007      0.000007   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000015      0.000016      0.000017      0.000019   \n",
       "50%    ...      0.000021      0.000021      0.000022      0.000023   \n",
       "75%    ...      0.000024      0.000024      0.000026      0.000027   \n",
       "max    ...      0.000048      0.000046      0.000049      0.000047   \n",
       "\n",
       "            B11_t-0       B12_t-0       et0_t-0      rain_t-0  cum_rain_t-0  \\\n",
       "count  12340.000000  12340.000000  1.234000e+04  1.234000e+04     12340.000   \n",
       "mean       0.000025      0.000019  4.420460e+00  3.440000e-01       321.342   \n",
       "std        0.000009      0.000007  8.882144e-16  5.551340e-17         0.000   \n",
       "min        0.000000      0.000000  4.420460e+00  3.440000e-01       321.342   \n",
       "25%        0.000020      0.000015  4.420460e+00  3.440000e-01       321.342   \n",
       "50%        0.000024      0.000018  4.420460e+00  3.440000e-01       321.342   \n",
       "75%        0.000030      0.000026  4.420460e+00  3.440000e-01       321.342   \n",
       "max        0.000047      0.000035  4.420460e+00  3.440000e-01       321.342   \n",
       "\n",
       "       Ertr.masse (Nass)(tonne/ha)  \n",
       "count                 12340.000000  \n",
       "mean                      6.642245  \n",
       "std                       2.099927  \n",
       "min                       0.286000  \n",
       "25%                       5.253000  \n",
       "50%                       6.691000  \n",
       "75%                       8.143000  \n",
       "max                      17.240000  \n",
       "\n",
       "[8 rows x 253 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df[feature_cols]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(out_df[feature_cols].values)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(out_df[feature_cols].values)\n",
    "x_scaled2 = scaler.transform(out_df[feature_cols].values)\n",
    "\n",
    "test = pd.DataFrame(columns=feature_cols, data=x_scaled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = filter_by_2std(mean, std, 'Ertr.masse (Nass)(tonne/ha)', out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mean train R2: 0.9430185736708638, individual folds: [0.94320968 0.94251857 0.94098539 0.94596446 0.94241477]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Mean test R2: 0.5900296355296978, individual folds: [0.59658498 0.59257612 0.59670335 0.57854101 0.58574272]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train RF with pixels and apply\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "rf = RandomForestRegressor(n_jobs=4, n_estimators=100)\n",
    "\n",
    "target_col = \"Ertr.masse (Nass)(tonne/ha)\"\n",
    "# feature_cols = list(range(0, 326))\n",
    "\n",
    "results = cross_validate(rf,\n",
    "                         X=out_df[feature_cols],\n",
    "                         y=out_df[target_col],\n",
    "                         cv=cv,\n",
    "                         scoring=('r2', 'neg_mean_squared_error'),\n",
    "                         return_train_score=True)\n",
    "\n",
    "display(\"Mean train R2: {}, individual folds: {}\".format(\n",
    "    np.mean(results[\"train_r2\"]), results[\"train_r2\"]))\n",
    "display(\"Mean test R2: {}, individual folds: {}\".format(\n",
    "    np.mean(results[\"test_r2\"]), results[\"test_r2\"]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "754299ca8a2537c9a9f489998c42930686a67643463836b55bb352a8eff7971c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
