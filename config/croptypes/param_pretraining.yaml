#configuration of the experiments

transformer:
  input_dim: 9
  num_classes: 6
  n_head: 4
  nlayers: 3

pretraining:
  batch_size: 256
  learning_rate: 0.0016612
  epochs: 2
  gpus: 0
  out_dim: 14 # definitions for simsiam
  pred_hidden_dim: 14 # definitions for simsiam
  proj_hidden_dim: 14 # definitions for simsiam
  num_ftrs: 64 # definitions for simsiam
  pe: False #positional encoding
  num_workers: 4
  device: 'cpu'
  no_gpus: 14 # if gpu available

finetuning:
  batch_size: 256
  learning_rate: 0.0016612
  epochs: 1









